{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encrypted DNA ancestry using Concrete ML by Horaizon27 team</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from src.utils import read_vcf, save_dict, vcf_to_npy, read_genetic_map\n",
    "from src.laidataset import LAIDataset\n",
    "from src.model import Gnomix\n",
    "from concrete_models import ConcreteGnomix\n",
    "from training_utils import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
    "GENETIC_MAP_FILE = \"data/allchrs.b37.gmap\"\n",
    "REFERENCE_FILE = \"data/reference_1000g.vcf\"\n",
    "SAMPLE_MAP_FILE = \"data/1000g.smap\"\n",
    "SINGLE_ANCESTRY_SAMPLES_FILE = \"data/samples_1000g.tsv\"\n",
    "WORKING_DIR = \"tmp\"\n",
    "TRAINING_DATA_DIR = os.path.join(WORKING_DIR, \"training_data\")\n",
    "CHM = \"22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preparation</h3>\n",
    "\n",
    "Almost all necessary files are stored in **/data** directory. \n",
    "However, some steps still need to be completed:\n",
    "1) Download **query file** and put it in **/data** directory\n",
    "2) Create **reference file** and put it in **/data** directory\n",
    "3) Load config file\n",
    "4) Generate trainning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading query file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "QUERY_FILE_URL = \"https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
    "urllib.request.urlretrieve(QUERY_FILE_URL, QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_map = pd.read_csv(SAMPLE_MAP_FILE, sep=\"\\t\")\n",
    "np.savetxt(\n",
    "    SINGLE_ANCESTRY_SAMPLES_FILE, list(sample_map[\"#Sample\"]), delimiter=\"\\t\", fmt=\"%s\"\n",
    ")\n",
    "cmd = \"bcftools view -S {} -o {} {}\".format(SINGLE_ANCESTRY_SAMPLES_FILE, REFERENCE_FILE, QUERY_FILE)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading models config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.UnsafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(\n",
    "        training_data_dir, reference, genetic_map, sample_map, chm, config, force_regeneration=False\n",
    "):\n",
    "    \"\"\" Training data generation\n",
    "    Imported almost as is from Gnomix repo (simulate_splits function in gnomix.py)\n",
    "    \"\"\"\n",
    "\n",
    "    r_admixed = config[\"simulation\"][\"r_admixed\"]\n",
    "    print (\n",
    "        \"Generating training data in {} with r_admixed: {}\".format(training_data_dir, r_admixed)\n",
    "    )\n",
    "    \n",
    "    if os.path.exists(training_data_dir):\n",
    "        if force_regeneration:\n",
    "            shutil.rmtree(training_data_dir)\n",
    "        else:\n",
    "            print (\"Training data already exists\", training_data_dir)\n",
    "            return\n",
    "    os.makedirs(training_data_dir)\n",
    "\n",
    "\n",
    "    laidataset = LAIDataset(chm, reference, genetic_map, seed=config[\"seed\"])\n",
    "    laidataset.buildDataset(sample_map)\n",
    "\n",
    "    sample_map_path = os.path.join(training_data_dir, \"sample_maps\")\n",
    "    os.makedirs(sample_map_path)\n",
    "\n",
    "    # split sample map and write it.\n",
    "    splits = config[\"simulation\"][\"splits\"][\"ratios\"]\n",
    "    if len(laidataset) <= 25:\n",
    "        if splits.get(\"val\"):\n",
    "            print(\"WARNING: Too few samples to run validation.\")\n",
    "            del config[\"simulation\"][\"splits\"][\"ratios\"][\"val\"]\n",
    "    laidataset.create_splits(splits, sample_map_path)\n",
    "\n",
    "    save_dict(laidataset.metadata(), os.path.join(training_data_dir, \"metadata.pkl\"))\n",
    "\n",
    "    # get num_outs\n",
    "    split_generations = config[\"simulation\"][\"splits\"][\"gens\"]\n",
    "    \n",
    "    num_outs = {}\n",
    "    min_splits = {\"train1\": 400, \"train2\": 75, \"val\": 25}\n",
    "    for split in splits:\n",
    "        total_sim = max(\n",
    "            len(laidataset.return_split(split)) * r_admixed,\n",
    "            min_splits[split]\n",
    "        )\n",
    "        num_outs[split] = int(total_sim / len(split_generations[split]))\n",
    "\n",
    "\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(training_data_dir, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            os.makedirs(split_path)\n",
    "        for gen in split_generations[split]:\n",
    "            laidataset.simulate(\n",
    "                num_outs[split],\n",
    "                split=split,\n",
    "                gen=gen,\n",
    "                outdir=os.path.join(split_path, \"gen_{}\".format(gen)),\n",
    "                return_out=False\n",
    "            )\n",
    "\n",
    "    print (\"Generated {} splits: {}\".format(len(splits), splits))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data in tmp/training_data with r_admixed: 0.6\n",
      "Reading vcf file...\n",
      "Getting genetic map info...\n",
      "Getting sample map info...\n",
      "Building founders...\n",
      "Splitting sample map...\n",
      "Generated 3 splits: {'train1': 0.8, 'train2': 0.15, 'val': 0.05}\n"
     ]
    }
   ],
   "source": [
    "generate_training_data(\n",
    "    TRAINING_DATA_DIR, REFERENCE_FILE, GENETIC_MAP_FILE, SAMPLE_MAP_FILE, CHM, config, force_regeneration=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Models</h2>\n",
    "\n",
    "We are going to train two models: Gnomix (Non-FHE) and ConcreteGnomix (FHE)<br>\n",
    "- Both models will be trained using the same **config.yaml** file that was downloaded from Gnomix repository\n",
    "- Both models will use the same generated training data\n",
    "\n",
    "More information about models you can find in **README.md**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Models training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, training_data_path, genetic_map_path, chm, model_type=\"concrete\", evaluate=False):\n",
    "    \"\"\" Model training\n",
    "    Creates and trains model depending on mode and config data\n",
    "    'default' mode: original Gnomix model with LogisticRegression and XGBClassifier\n",
    "    'concrete' mode: similar to Gnomix model with concrete versions of LogisticRegression and XGBClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    window_size_cM=config[\"model\"].get(\"window_size_cM\")\n",
    "    smooth_window_size=config[\"model\"].get(\"smooth_size\")\n",
    "    n_cores=config[\"model\"].get(\"n_cores\", None)\n",
    "    retrain_base=config[\"model\"].get(\"retrain_base\")\n",
    "    calibrate=config[\"model\"].get(\"calibrate\")\n",
    "    context_ratio=config[\"model\"].get(\"context_ratio\")\n",
    "    generations = config[\"simulation\"][\"splits\"][\"gens\"]\n",
    "\n",
    "    print(\"Reading training data...\")\n",
    "    data, meta = get_data(training_data_path, generations, window_size_cM, model_type)\n",
    "\n",
    "    if model_type == \"concrete\":\n",
    "        print(\"Training Concrete model...\")\n",
    "        model = ConcreteGnomix(\n",
    "            C=meta[\"C\"], M=meta[\"M\"], A=meta[\"A\"], S=smooth_window_size,\n",
    "            snp_pos=meta[\"snp_pos\"], snp_ref=meta[\"snp_ref\"], snp_alt=meta[\"snp_alt\"],\n",
    "            population_order=meta[\"pop_order\"], calibrate=calibrate,\n",
    "            n_jobs=n_cores, context_ratio=context_ratio, seed=config[\"seed\"],\n",
    "        )\n",
    "        model.train(data=data, retrain_base=retrain_base, evaluate=evaluate, compile=False)\n",
    "\n",
    "    elif model_type == \"gnomix\":\n",
    "        print(\"Training Gnomix model...\")\n",
    "        model = Gnomix(\n",
    "            C=meta[\"C\"], M=meta[\"M\"], A=meta[\"A\"], S=smooth_window_size,\n",
    "            snp_pos=meta[\"snp_pos\"], snp_ref=meta[\"snp_ref\"], snp_alt=meta[\"snp_alt\"],\n",
    "            population_order=meta[\"pop_order\"], calibrate=calibrate,\n",
    "            n_jobs=n_cores, context_ratio=context_ratio, seed=config[\"seed\"],\n",
    "        )\n",
    "        model.train(data=data, retrain_base=retrain_base, evaluate=evaluate)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Unknown model type: {}\".format(model_type))\n",
    "\n",
    "    # write gentic map df to model variable\n",
    "    model.write_gen_map_df(\n",
    "        read_genetic_map(genetic_map_path, chm)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gnomix model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Training Gnomix model...\n",
      "Training base models...\n",
      "Training smoother...\n",
      "Evaluating model...\n",
      "training accuracy\n",
      "val accuracy\n",
      "Re-training base models...\n"
     ]
    }
   ],
   "source": [
    "gnomix_model = train_model(config, TRAINING_DATA_DIR, GENETIC_MAP_FILE, CHM, model_type=\"gnomix\", evaluate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Training Concrete model...\n",
      "Training base model...\n",
      "Training smoother...\n",
      "Evaluating model...\n",
      "training accuracy\n",
      "val accuracy\n",
      "Re-training base models...\n",
      "Base model compile\n",
      "Smooth model compile\n"
     ]
    }
   ],
   "source": [
    "concrete_model = train_model(config, TRAINING_DATA_DIR, GENETIC_MAP_FILE, CHM, model_type=\"concrete\", evaluate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference time comparison</h2>\n",
    "\n",
    "For comparison, we will try three options:\n",
    "\n",
    "1) Default Gnomix model (non-FHE)\n",
    "2) ConcreteGnomix model, which uses **FHE simulation** at both stages (sim-FHE)\n",
    "3) ConcreteGnomix model, which uses **FHE** only at the first stage (half-FHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(model, query_vcf_data, fhe_data=None):\n",
    "    \n",
    "    # preparing data\n",
    "    X_query, _, _ = vcf_to_npy(\n",
    "        query_vcf_data, model.snp_pos, model.snp_ref, return_idx=True, verbose=False\n",
    "    )\n",
    "    samples = query_vcf_data[\"samples\"]\n",
    "\n",
    "    predictions_start = time.time()\n",
    "    # making predictions\n",
    "    if fhe_data is None:\n",
    "        B_query = model.base.predict_proba(X_query)\n",
    "        y_proba_query = model.smooth.predict_proba(B_query)\n",
    "    else: \n",
    "        B_query = model.base.predict_proba(X_query, fhe=fhe_data[\"base\"])\n",
    "        y_proba_query = model.smooth.predict_proba(B_query, fhe=fhe_data[\"smooth\"])\n",
    "    y_pred_query = np.argmax(y_proba_query, axis=-1)\n",
    "\n",
    "    # getting final prediction\n",
    "    ind_idx = np.arange(0, len(y_pred_query), 2) \n",
    "    final_prediction = stats.mode(y_pred_query[ind_idx,:], axis=1).mode\n",
    "\n",
    "    predictions_end = time.time()\n",
    "    avg_time = (predictions_end - predictions_start) / len(samples)\n",
    "    print(\"Average inference time per sample: {}\".format(avg_time))\n",
    "\n",
    "    predictions = {\n",
    "        sample_name: int(prediction)\n",
    "        for sample_name, prediction in zip(samples, final_prediction)\n",
    "    }\n",
    "\n",
    "    return {\"predictions\": predictions, \"avg_inference_time\": avg_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_NUM = 10\n",
    "\n",
    "query_samples = pd.read_csv(SAMPLE_MAP_FILE, sep=\"\\t\").sample(QUERIES_NUM).set_index(\"#Sample\").to_dict()['Panel']\n",
    "query_vcf_data = read_vcf(QUERY_FILE, chm=CHM, fields=\"*\", samples=query_samples.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gnomix model (Non-FHE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample: 0.9127118825912476\n"
     ]
    }
   ],
   "source": [
    "gnomix_inference_result = get_inference(gnomix_model, query_vcf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete model in simulation mode (sim-FHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample: 19.036097359657287\n"
     ]
    }
   ],
   "source": [
    "fhe_data = {\"base\": \"simulate\", \"smooth\": \"simulate\"}\n",
    "concrete_sim_inference_result = get_inference(concrete_model, query_vcf_data, fhe_data=fhe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete model with FHE only at first stage   (half-FHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample: 826.38837920489296635\n"
     ]
    }
   ],
   "source": [
    "fhe_data = {\"base\": \"execute\", \"smooth\": \"disable\"}\n",
    "concrete_half_inference_result = get_inference(concrete_model, query_vcf_data, fhe_data=fhe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-FHE</th>\n",
       "      <th>Sim-FHE</th>\n",
       "      <th>Half-FHE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>97.750000</td>\n",
       "      <td>97.260000</td>\n",
       "      <td>97.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inference time</th>\n",
       "      <td>0.912712</td>\n",
       "      <td>19.036097</td>\n",
       "      <td>826.388379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Non-FHE    Sim-FHE    Half-FHE\n",
       "Accuracy        97.750000  97.260000   97.260000\n",
       "Inference time   0.912712  19.036097  826.388379"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            gnomix_model.accuracies[\"smooth_val_acc_bal\"],\n",
    "            concrete_model.accuracies[\"smooth_val_acc_bal\"],\n",
    "            concrete_model.accuracies[\"smooth_val_acc_bal\"]\n",
    "        ],\n",
    "        [\n",
    "            gnomix_inference_result['avg_inference_time'],\n",
    "            concrete_sim_inference_result['avg_inference_time'],\n",
    "            concrete_half_inference_result['avg_inference_time']\n",
    "        ]\n",
    "    ], \n",
    "    index=[\"Accuracy\", \"Inference time\"], \n",
    "    columns=[\"Non-FHE\", \"Sim-FHE\", \"Half-FHE\"]\n",
    ")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
